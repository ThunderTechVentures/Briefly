{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.8/5.6 MB 14.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 7.3 MB/s eta 0:00:00\n",
      "Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, charset-normalizer, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-10.4.0 cffi-1.17.1 charset-normalizer-3.4.0 cryptography-43.0.1 pdfminer.six-20231228 pdfplumber-0.11.4 pycparser-2.22 pypdfium2-4.30.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to ../data/data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to your PDF file and the output JSON file\n",
    "pdf_path = '../data/india_constitution.pdf'\n",
    "json_path = '../data/data.json'\n",
    "\n",
    "# Initialize a dictionary to hold structured data\n",
    "structured_data = {\n",
    "    \"pages\": []\n",
    "}\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    # Loop through each page in the PDF\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        # Extract text from the page\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            # Create a dictionary for the current page\n",
    "            page_data = {\n",
    "                \"page_number\": page_number,\n",
    "                \"section\": None,  # You can add logic to extract section titles if available\n",
    "                \"title\": None,    # You can add logic to extract titles if available\n",
    "                \"text\": text,\n",
    "                \"keywords\": []    # Populate this with relevant keywords if possible\n",
    "            }\n",
    "            # Append the page data to the structured data\n",
    "            structured_data[\"pages\"].append(page_data)\n",
    "\n",
    "# Save the structured data to a JSON file\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(structured_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Extracted text saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
